[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog was created for the Applied Bioinformatics course in 2025, where I posted everyday updates during week 41:))"
  },
  {
    "objectID": "posts/Day1/index.html",
    "href": "posts/Day1/index.html",
    "title": "Day 1",
    "section": "",
    "text": "On Monday, we covered some topics relating to good data practice, git and quarto. At the end of the day, we set ourselves up with individual blogs for documenting course progress.\n\n\n\nIntroduction • Course content, schedule etc • People intros\nData management Data cycle FAIR principles Good data management practices • Research documentation • Data organisation • Information security • Ethics and legislation (Sweden: research data needs to be kept for 10 years)\n\nData sets: central dogma of biology &lt;-&gt; bioinformatics\nBest practices • Raw data in separate directory • Code in another directory • Output (figs) in separate directory • Version control • README in every directory • File naming that is easy to understand for humans and machines (no ö, spaces characters) • Use non-proprietary formats —&gt; .csv instead of .xlsx\n\nLiterate programming • Code chunks • Markdown Notebook in quarto\nVersion control Git and good git practices Clear and informative, commit often (multiple times per day - once per day) —&gt; Writing blog posts every day\nEnvironment managers Using pixi in this course\nContainers Includes everything necessary to run code and do the necessary analysis, including the OS.\nWorkflow manager —&gt; Nextflow\n\n\n\n\n• git branch • git checkout • git merge • git diff • git add • git commit -m “Message” Commit on a single theme, message should be in imperative • git push\nNotes on branching • git branch —&gt; creates a new branch based on the current commit\nMerging • specific branch or specific commit • conflicts need to be resolved\nAdd and commit • add —&gt; taking area • commit —&gt; send to repo\nPush, pull and collaborate • git push origin main\nExercise: alphabetise a list • Format first name, last name • Branching • Merge with neighbours via commit, pull, merge and push until 14 names are in alphabetical order."
  },
  {
    "objectID": "posts/Day1/index.html#intro-to-course-by-amrei",
    "href": "posts/Day1/index.html#intro-to-course-by-amrei",
    "title": "Day 1",
    "section": "",
    "text": "Introduction • Course content, schedule etc • People intros\nData management Data cycle FAIR principles Good data management practices • Research documentation • Data organisation • Information security • Ethics and legislation (Sweden: research data needs to be kept for 10 years)\n\nData sets: central dogma of biology &lt;-&gt; bioinformatics\nBest practices • Raw data in separate directory • Code in another directory • Output (figs) in separate directory • Version control • README in every directory • File naming that is easy to understand for humans and machines (no ö, spaces characters) • Use non-proprietary formats —&gt; .csv instead of .xlsx\n\nLiterate programming • Code chunks • Markdown Notebook in quarto\nVersion control Git and good git practices Clear and informative, commit often (multiple times per day - once per day) —&gt; Writing blog posts every day\nEnvironment managers Using pixi in this course\nContainers Includes everything necessary to run code and do the necessary analysis, including the OS.\nWorkflow manager —&gt; Nextflow"
  },
  {
    "objectID": "posts/Day1/index.html#git-and-github-by-samuel-flores",
    "href": "posts/Day1/index.html#git-and-github-by-samuel-flores",
    "title": "Day 1",
    "section": "",
    "text": "• git branch • git checkout • git merge • git diff • git add • git commit -m “Message” Commit on a single theme, message should be in imperative • git push\nNotes on branching • git branch —&gt; creates a new branch based on the current commit\nMerging • specific branch or specific commit • conflicts need to be resolved\nAdd and commit • add —&gt; taking area • commit —&gt; send to repo\nPush, pull and collaborate • git push origin main\nExercise: alphabetise a list • Format first name, last name • Branching • Merge with neighbours via commit, pull, merge and push until 14 names are in alphabetical order."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "How to make a quarto blog",
    "section": "",
    "text": "This is my first post in my Quarto blog. Welcome!\n There is no content in this post."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Bioinformatics Blog",
    "section": "",
    "text": "MedBioInfo 2025\n\n\n\npeople\n\n\n\nThe group\n\n\n\n\n\nOct 11, 2025\n\n\nJoel Tekoniemi\n\n\n\n\n\n\n\n\n\n\n\n\nDay 5\n\n\n\ndiary\n\n\n\nVisualise your data with ggplot2\n\n\n\n\n\nOct 10, 2025\n\n\nJoel Tekoniemi\n\n\n\n\n\n\n\n\n\n\n\n\nDay 4\n\n\n\ndiary\n\n\n\nSetting up a Nextflow rna-seq pipeline from nf-core\n\n\n\n\n\nOct 9, 2025\n\n\nJoel Tekoniemi\n\n\n\n\n\n\n\n\n\n\n\n\nDay 3\n\n\n\ndiary\n\n\n\nIntroduction to workflows and workflow managers\n\n\n\n\n\nOct 8, 2025\n\n\nJoel Tekoniemi\n\n\n\n\n\n\n\n\n\n\n\n\nDay 2\n\n\n\ndiary\n\n\n\nJob scheduling and container setup on remote cluster\n\n\n\n\n\nOct 7, 2025\n\n\nJoel Tekoniemi\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1\n\n\n\ndiary\n\n\n\nIntro to course and best practices for reproducible code\n\n\n\n\n\nOct 6, 2025\n\n\nJoel Tekoniemi\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Day2/index.html",
    "href": "posts/Day2/index.html",
    "title": "Day 2",
    "section": "",
    "text": "Today we set up some pixi environments and used slurm to submit batch jobs on the hpc2n cluster. Then, we explored the use of containers with apptainer. Everything done today was on the hpc2n remote cluster.\n\n\nThe pixi environments are installed into folders and called using pixi run &lt;...&gt;. It’s pretty simple and new packages can be added to the environment using pixi add &lt;package name&gt;. If an environment is not needed anymore, we can delete the folder inside which the environment was created.\n\n\n\nOn the cluster, we can submit jobs to be run with automatic allocation of resources so that we can focus on other things.\nRunning pixi with slurm:\nsrun -A &lt;PROJECT-ID&gt; -t 15:00 -n 1 pixi run fastqc  --noextract -o fastqc &lt;PATH&gt;/joel/RNAseq-data/*.fastq.gz\nIn this case, we used an example data set of RNA-seq files and ran fastqc. The dataset is accessible at PRJNA369563\n\n\n\nFor this course, we used apptainer to run and build containers. Ready-made (rocommended) containers can be obtained from dockerhub or seqera, and in this case we used both. First, we pulled the vcftools container, after which we pulled fastqc in order to re-run qc on the sequencing reads above, but using a container and batch script. Running fastqc in a container: apptainer exec ../containers/fastqc_0.12.1.sif fastqc -o ../fastqc-container --noextract ../*fastq.gz\n\n\nUsing a definition file, containers can be built relatively simply. When the container is run using the run command, the code under the %runscript block will be run automatically. This can be overruled with the exec comand.\nExamples:\n$ apptainer run containers/lolcow.sif\n$ apptainer exec containers/lolcow.sif \"date|cowsay\"\nThis is the peak of the day’s accomplishments, and we enjoy the company of our inspirational cow :). Here, have my favourite inspirational quote (after a couple iterations):"
  },
  {
    "objectID": "posts/Day2/index.html#job-scheduling-using-slurm",
    "href": "posts/Day2/index.html#job-scheduling-using-slurm",
    "title": "Day 2",
    "section": "",
    "text": "On the cluster, we can submit jobs to be run with automatic allocation of resources so that we can focus on other things.\nRunning pixi with slurm:\nsrun -A &lt;PROJECT-ID&gt; -t 15:00 -n 1 pixi run fastqc  --noextract -o fastqc &lt;PATH&gt;/joel/RNAseq-data/*.fastq.gz\nIn this case, we used an example data set of RNA-seq files and ran fastqc. The dataset is accessible at PRJNA369563"
  },
  {
    "objectID": "posts/Day2/index.html#containers",
    "href": "posts/Day2/index.html#containers",
    "title": "Day 2",
    "section": "",
    "text": "For this course, we used apptainer to run and build containers. Ready-made (rocommended) containers can be obtained from dockerhub or seqera, and in this case we used both. First, we pulled the vcftools container, after which we pulled fastqc in order to re-run qc on the sequencing reads above, but using a container and batch script. Running fastqc in a container: apptainer exec ../containers/fastqc_0.12.1.sif fastqc -o ../fastqc-container --noextract ../*fastq.gz\n\n\nUsing a definition file, containers can be built relatively simply. When the container is run using the run command, the code under the %runscript block will be run automatically. This can be overruled with the exec comand.\nExamples:\n$ apptainer run containers/lolcow.sif\n$ apptainer exec containers/lolcow.sif \"date|cowsay\"\nThis is the peak of the day’s accomplishments, and we enjoy the company of our inspirational cow :). Here, have my favourite inspirational quote (after a couple iterations):"
  },
  {
    "objectID": "posts/Day2/index.html#pixi",
    "href": "posts/Day2/index.html#pixi",
    "title": "Day 2",
    "section": "",
    "text": "The pixi environments are installed into folders and called using pixi run &lt;...&gt;. It’s pretty simple and new packages can be added to the environment using pixi add &lt;package name&gt;. If an environment is not needed anymore, we can delete the folder inside which the environment was created."
  },
  {
    "objectID": "posts/Day4/Day4.html",
    "href": "posts/Day4/Day4.html",
    "title": "Day 4",
    "section": "",
    "text": "It is pretty easy to setup nextflow pipelines with the help of nf-core, but there is nonetheless some configuration that is needed: - Create Pixi environment - add channels conda-forge and bioconda -c conda-forge -c bioconda - add packages nextflow and nf-core - Create data folder and other necessary folder for pipeline E.g. genome, results, Symbolic link nl for data to avoid copying - Navigate to the nf-core pipeline and use the installer\nExample using the rna-seq pipeline\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\n\n\n\nImage: rna-seq-pipeline\n\n\n\n\n\n\ntags: tags are shown next to processes when running pipeline\nworkflow.onComplete: more info and user-friendly messages upon completed workflow"
  },
  {
    "objectID": "posts/Day4/Day4.html#pipeline-setup",
    "href": "posts/Day4/Day4.html#pipeline-setup",
    "title": "Day 4",
    "section": "",
    "text": "It is pretty easy to setup nextflow pipelines with the help of nf-core, but there is nonetheless some configuration that is needed: - Create Pixi environment - add channels conda-forge and bioconda -c conda-forge -c bioconda - add packages nextflow and nf-core - Create data folder and other necessary folder for pipeline E.g. genome, results, Symbolic link nl for data to avoid copying - Navigate to the nf-core pipeline and use the installer\nExample using the rna-seq pipeline\npixi run nextflow run nf-core/rnaseq -r 3.19.0 -resume -params-file nf-params.json -c hpc2n.config\n\n\n\nImage: rna-seq-pipeline"
  },
  {
    "objectID": "posts/Day4/Day4.html#fancy-stuff",
    "href": "posts/Day4/Day4.html#fancy-stuff",
    "title": "Day 4",
    "section": "",
    "text": "tags: tags are shown next to processes when running pipeline\nworkflow.onComplete: more info and user-friendly messages upon completed workflow"
  },
  {
    "objectID": "posts/Day2/Day2.html",
    "href": "posts/Day2/Day2.html",
    "title": "Day 2",
    "section": "",
    "text": "Today we set up some pixi environments and used slurm to submit batch jobs on the hpc2n cluster. Then, we explored the use of containers with apptainer. Everything done today was on the hpc2n remote cluster.\n\n\nThe pixi environments are installed into folders and called using pixi run &lt;...&gt;. It’s pretty simple and new packages can be added to the environment using pixi add &lt;package name&gt;. If an environment is not needed anymore, we can delete the folder inside which the environment was created.\n\n\n\nOn the cluster, we can submit jobs to be run with automatic allocation of resources so that we can focus on other things.\nRunning pixi with slurm:\nsrun -A &lt;PROJECT-ID&gt; -t 15:00 -n 1 pixi run fastqc  --noextract -o fastqc &lt;PATH&gt;/joel/RNAseq-data/*.fastq.gz\nIn this case, we used an example data set of RNA-seq files and ran fastqc. The dataset is accessible at PRJNA369563\n\n\n\nFor this course, we used apptainer to run and build containers. Ready-made (rocommended) containers can be obtained from dockerhub or seqera, and in this case we used both. First, we pulled the vcftools container, after which we pulled fastqc in order to re-run qc on the sequencing reads above, but using a container and batch script. Running fastqc in a container: apptainer exec ../containers/fastqc_0.12.1.sif fastqc -o ../fastqc-container --noextract ../*fastq.gz\n\n\nUsing a definition file, containers can be built relatively simply. When the container is run using the run command, the code under the %runscript block will be run automatically. This can be overruled with the exec comand.\nExamples:\n$ apptainer run containers/lolcow.sif\n$ apptainer exec containers/lolcow.sif \"date|cowsay\"\nThis is the peak of the day’s accomplishments, and we enjoy the company of our inspirational cow :). Here, have my favourite inspirational quote (after a couple iterations):"
  },
  {
    "objectID": "posts/Day2/Day2.html#pixi",
    "href": "posts/Day2/Day2.html#pixi",
    "title": "Day 2",
    "section": "",
    "text": "The pixi environments are installed into folders and called using pixi run &lt;...&gt;. It’s pretty simple and new packages can be added to the environment using pixi add &lt;package name&gt;. If an environment is not needed anymore, we can delete the folder inside which the environment was created."
  },
  {
    "objectID": "posts/Day2/Day2.html#job-scheduling-using-slurm",
    "href": "posts/Day2/Day2.html#job-scheduling-using-slurm",
    "title": "Day 2",
    "section": "",
    "text": "On the cluster, we can submit jobs to be run with automatic allocation of resources so that we can focus on other things.\nRunning pixi with slurm:\nsrun -A &lt;PROJECT-ID&gt; -t 15:00 -n 1 pixi run fastqc  --noextract -o fastqc &lt;PATH&gt;/joel/RNAseq-data/*.fastq.gz\nIn this case, we used an example data set of RNA-seq files and ran fastqc. The dataset is accessible at PRJNA369563"
  },
  {
    "objectID": "posts/Day2/Day2.html#containers",
    "href": "posts/Day2/Day2.html#containers",
    "title": "Day 2",
    "section": "",
    "text": "For this course, we used apptainer to run and build containers. Ready-made (rocommended) containers can be obtained from dockerhub or seqera, and in this case we used both. First, we pulled the vcftools container, after which we pulled fastqc in order to re-run qc on the sequencing reads above, but using a container and batch script. Running fastqc in a container: apptainer exec ../containers/fastqc_0.12.1.sif fastqc -o ../fastqc-container --noextract ../*fastq.gz\n\n\nUsing a definition file, containers can be built relatively simply. When the container is run using the run command, the code under the %runscript block will be run automatically. This can be overruled with the exec comand.\nExamples:\n$ apptainer run containers/lolcow.sif\n$ apptainer exec containers/lolcow.sif \"date|cowsay\"\nThis is the peak of the day’s accomplishments, and we enjoy the company of our inspirational cow :). Here, have my favourite inspirational quote (after a couple iterations):"
  },
  {
    "objectID": "posts/Day3/Day3.html",
    "href": "posts/Day3/Day3.html",
    "title": "Day 3",
    "section": "",
    "text": "Workflow / pipeline = many scripts (usually one per tool), deployed one after the other\nWorkflow managers help to connect scripts in a pipeline, with automatic control over resource allocation and error management, e.g. re-submitting a batch job with double memory if it failed.\n\n\nOpen-source workflow manager. Channels: contain data, input / output Process: scripts\nQueue channel: unidirectional FIFO queue, can be read only once in the pipeline\nValue channel: can be read multiple times\nExecution abstraction\n\n\nsrun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data data/sample_1.fastq.gz data/sample_2.fastq.gz\n—&gt; mix of information about command and info about script in the same line In Nextflow, these are separate. Executor: determines how the script is run in the target platform\n\n\n\n\n\nAdding variables into channel —&gt; Channel.of()\nDefining process blocks Channel operators can be used on channels Input can be value, file, path, etc. —&gt; the variable type is specified Output is similar, can also be “stdout” which is just the terminal output\nWorkflow block\n\nModify and resume Runs are cached, and the output can be retrieved using the -resume flag, instead of rerunning the whole script. Double-dashes can be specified to change nf process parameters: –greeting ‘Bonjour le monde’ —&gt; changes params.greeting.\n\n\n\nnextflow log: see run history nextflow clean: deletes project cache and working directories. -before: cleans up previous runs pixi run nextflow clean -before  -f\n\n\n\nExecutor setup in nextflow.config Processes: slurm as executor + time, cpus, etc. Other statements: • Resume • Singularity containers • Executor account: E.g. HPC2N\n\n\n\nCommunity nextflow pipelines with extensive documentation.\nInteresting pipelines • rnaseq: classic RNA-seq, provides gene expression matrix as output • pixelator: Pixelgen MPX/PNA data • raredisease: variant calling and scoring from WGS/WES from rare disease patients"
  },
  {
    "objectID": "posts/Day3/Day3.html#nextflow",
    "href": "posts/Day3/Day3.html#nextflow",
    "title": "Day 3",
    "section": "",
    "text": "Open-source workflow manager. Channels: contain data, input / output Process: scripts\nQueue channel: unidirectional FIFO queue, can be read only once in the pipeline\nValue channel: can be read multiple times\nExecution abstraction\n\n\nsrun -A project_ID -t 15:00 -n 1 fastqc --noextract -o fastqc data data/sample_1.fastq.gz data/sample_2.fastq.gz\n—&gt; mix of information about command and info about script in the same line In Nextflow, these are separate. Executor: determines how the script is run in the target platform"
  },
  {
    "objectID": "posts/Day3/Day3.html#netflow-scripts",
    "href": "posts/Day3/Day3.html#netflow-scripts",
    "title": "Day 3",
    "section": "",
    "text": "Adding variables into channel —&gt; Channel.of()\nDefining process blocks Channel operators can be used on channels Input can be value, file, path, etc. —&gt; the variable type is specified Output is similar, can also be “stdout” which is just the terminal output\nWorkflow block\n\nModify and resume Runs are cached, and the output can be retrieved using the -resume flag, instead of rerunning the whole script. Double-dashes can be specified to change nf process parameters: –greeting ‘Bonjour le monde’ —&gt; changes params.greeting."
  },
  {
    "objectID": "posts/Day3/Day3.html#cleanup",
    "href": "posts/Day3/Day3.html#cleanup",
    "title": "Day 3",
    "section": "",
    "text": "nextflow log: see run history nextflow clean: deletes project cache and working directories. -before: cleans up previous runs pixi run nextflow clean -before  -f"
  },
  {
    "objectID": "posts/Day3/Day3.html#rna-seq-pipeline",
    "href": "posts/Day3/Day3.html#rna-seq-pipeline",
    "title": "Day 3",
    "section": "",
    "text": "Executor setup in nextflow.config Processes: slurm as executor + time, cpus, etc. Other statements: • Resume • Singularity containers • Executor account: E.g. HPC2N"
  },
  {
    "objectID": "posts/Day3/Day3.html#nf-core",
    "href": "posts/Day3/Day3.html#nf-core",
    "title": "Day 3",
    "section": "",
    "text": "Community nextflow pipelines with extensive documentation.\nInteresting pipelines • rnaseq: classic RNA-seq, provides gene expression matrix as output • pixelator: Pixelgen MPX/PNA data • raredisease: variant calling and scoring from WGS/WES from rare disease patients"
  },
  {
    "objectID": "posts/Day1/Day1.html",
    "href": "posts/Day1/Day1.html",
    "title": "Day 1",
    "section": "",
    "text": "On Monday, we covered some topics relating to good data practice, git and quarto. At the end of the day, we set ourselves up with individual blogs for documenting course progress.\n\n\n\nIntroduction • Course content, schedule etc • People intros\nData management Data cycle FAIR principles Good data management practices • Research documentation • Data organisation • Information security • Ethics and legislation (Sweden: research data needs to be kept for 10 years)\n\nData sets: central dogma of biology &lt;-&gt; bioinformatics\nBest practices • Raw data in separate directory • Code in another directory • Output (figs) in separate directory • Version control • README in every directory • File naming that is easy to understand for humans and machines (no ö, spaces characters) • Use non-proprietary formats —&gt; .csv instead of .xlsx\n\nLiterate programming • Code chunks • Markdown Notebook in quarto\nVersion control Git and good git practices Clear and informative, commit often (multiple times per day - once per day) —&gt; Writing blog posts every day\nEnvironment managers Using pixi in this course\nContainers Includes everything necessary to run code and do the necessary analysis, including the OS.\nWorkflow manager —&gt; Nextflow\n\n\n\n\n• git branch • git checkout • git merge • git diff • git add • git commit -m “Message” Commit on a single theme, message should be in imperative • git push\nNotes on branching • git branch —&gt; creates a new branch based on the current commit\nMerging • specific branch or specific commit • conflicts need to be resolved\nAdd and commit • add —&gt; taking area • commit —&gt; send to repo\nPush, pull and collaborate • git push origin main\nExercise: alphabetise a list • Format first name, last name • Branching • Merge with neighbours via commit, pull, merge and push until 14 names are in alphabetical order."
  },
  {
    "objectID": "posts/Day1/Day1.html#intro-to-course-by-amrei",
    "href": "posts/Day1/Day1.html#intro-to-course-by-amrei",
    "title": "Day 1",
    "section": "",
    "text": "Introduction • Course content, schedule etc • People intros\nData management Data cycle FAIR principles Good data management practices • Research documentation • Data organisation • Information security • Ethics and legislation (Sweden: research data needs to be kept for 10 years)\n\nData sets: central dogma of biology &lt;-&gt; bioinformatics\nBest practices • Raw data in separate directory • Code in another directory • Output (figs) in separate directory • Version control • README in every directory • File naming that is easy to understand for humans and machines (no ö, spaces characters) • Use non-proprietary formats —&gt; .csv instead of .xlsx\n\nLiterate programming • Code chunks • Markdown Notebook in quarto\nVersion control Git and good git practices Clear and informative, commit often (multiple times per day - once per day) —&gt; Writing blog posts every day\nEnvironment managers Using pixi in this course\nContainers Includes everything necessary to run code and do the necessary analysis, including the OS.\nWorkflow manager —&gt; Nextflow"
  },
  {
    "objectID": "posts/Day1/Day1.html#git-and-github-by-samuel-flores",
    "href": "posts/Day1/Day1.html#git-and-github-by-samuel-flores",
    "title": "Day 1",
    "section": "",
    "text": "• git branch • git checkout • git merge • git diff • git add • git commit -m “Message” Commit on a single theme, message should be in imperative • git push\nNotes on branching • git branch —&gt; creates a new branch based on the current commit\nMerging • specific branch or specific commit • conflicts need to be resolved\nAdd and commit • add —&gt; taking area • commit —&gt; send to repo\nPush, pull and collaborate • git push origin main\nExercise: alphabetise a list • Format first name, last name • Branching • Merge with neighbours via commit, pull, merge and push until 14 names are in alphabetical order."
  },
  {
    "objectID": "posts/Day5/Day5.html",
    "href": "posts/Day5/Day5.html",
    "title": "Day 5",
    "section": "",
    "text": "By Abu Bakar Siddique, adapted from RaukR workshop at NBIS\n\n\nBase R or ggplot2\n\n\n• Consistent coding • Flexible • More complicated syntax for simple plots, easier for complex plots • Saved to object\n\n\n\n\nLeland Wilkinson’s The Grammar of Graphics —&gt; Hadley Wickham ggplot2 in 2005 \n\n\n• Data: input data always in data.frame format —&gt; str() to get structure of data • Aesthetic: mapping or visual characteristics of the geometry • Geometries: geometry representing data - points, lines, … • Facets: split plot into subplot • Statistics: statistical transformation- counts, means, … • Coordinates: numeric system to determine position of geometry - cartesian, polar, … • Scale: how visual characteristics are converted to display values • Theme: citrons points of display - font size, background colour, …\n\n\n\nWide data: every variable as separate column. Long data: all numeric variables are in one column, also called tidy data. —&gt; column names into one column: variable —&gt; values into values column\n\n\n\nStats can be plotted using “geom” argument in stat function call. All stats have default geometries.\n\n\n\nMapping: aesthetics mapped to variable Parameter: defined, set values\n\n\n\nControl positions, colour, fill, size, shape, alpha, linetype Syntax: scale_ Manual: scalemanual Continuous: scale press TAB —&gt; gradient, … Axes: scale__\n\n\n\nSplit to subplot based on variable(s) Faceting in one dimension: facet_wrap(~variable) Two dimensions: facet_grid(variable_1 ~ variable_2) / facet_grid(var_1 + var_2)\n\n\n\nEx: Cartesian, map, polar\n\n\n\nModify non-data plot elements/appearance • Save an appearance for later reuse • ?theme —&gt; theme_grey(), theme_bw() • Theme legend: control legend position theme(legend.) • Titles: labs() • Theme text: theme(axis.title = element_text(colour = …, size = …))\n\n\n\n\n• Theme rect: element_rect() • Theme reuse newtheme = theme_bw() + theme(…) p + new theme • Saving plots Function ggsave() • Combining plots Patchwork package patchwork::wrap_plots(p, q) + plot_annotation(tag_levels = “a”)\n\n\n\nsessionInfo() Lists dependencies, versions and useful information when publishing data."
  },
  {
    "objectID": "posts/Day5/Day5.html#graphs-to-show-data",
    "href": "posts/Day5/Day5.html#graphs-to-show-data",
    "title": "Day 5",
    "section": "",
    "text": "Base R or ggplot2\n\n\n• Consistent coding • Flexible • More complicated syntax for simple plots, easier for complex plots • Saved to object"
  },
  {
    "objectID": "posts/Day5/Day5.html#grammar-of-graphics",
    "href": "posts/Day5/Day5.html#grammar-of-graphics",
    "title": "Day 5",
    "section": "",
    "text": "Leland Wilkinson’s The Grammar of Graphics —&gt; Hadley Wickham ggplot2 in 2005 \n\n\n• Data: input data always in data.frame format —&gt; str() to get structure of data • Aesthetic: mapping or visual characteristics of the geometry • Geometries: geometry representing data - points, lines, … • Facets: split plot into subplot • Statistics: statistical transformation- counts, means, … • Coordinates: numeric system to determine position of geometry - cartesian, polar, … • Scale: how visual characteristics are converted to display values • Theme: citrons points of display - font size, background colour, …\n\n\n\nWide data: every variable as separate column. Long data: all numeric variables are in one column, also called tidy data. —&gt; column names into one column: variable —&gt; values into values column\n\n\n\nStats can be plotted using “geom” argument in stat function call. All stats have default geometries.\n\n\n\nMapping: aesthetics mapped to variable Parameter: defined, set values\n\n\n\nControl positions, colour, fill, size, shape, alpha, linetype Syntax: scale_ Manual: scalemanual Continuous: scale press TAB —&gt; gradient, … Axes: scale__\n\n\n\nSplit to subplot based on variable(s) Faceting in one dimension: facet_wrap(~variable) Two dimensions: facet_grid(variable_1 ~ variable_2) / facet_grid(var_1 + var_2)\n\n\n\nEx: Cartesian, map, polar\n\n\n\nModify non-data plot elements/appearance • Save an appearance for later reuse • ?theme —&gt; theme_grey(), theme_bw() • Theme legend: control legend position theme(legend.) • Titles: labs() • Theme text: theme(axis.title = element_text(colour = …, size = …))"
  },
  {
    "objectID": "posts/Day5/Day5.html#other-things-about-ggplot2",
    "href": "posts/Day5/Day5.html#other-things-about-ggplot2",
    "title": "Day 5",
    "section": "",
    "text": "• Theme rect: element_rect() • Theme reuse newtheme = theme_bw() + theme(…) p + new theme • Saving plots Function ggsave() • Combining plots Patchwork package patchwork::wrap_plots(p, q) + plot_annotation(tag_levels = “a”)"
  },
  {
    "objectID": "posts/Day5/Day5.html#other-things-about-r",
    "href": "posts/Day5/Day5.html#other-things-about-r",
    "title": "Day 5",
    "section": "",
    "text": "sessionInfo() Lists dependencies, versions and useful information when publishing data."
  },
  {
    "objectID": "posts/MedBioInfo/MedBioInfo2025.html",
    "href": "posts/MedBioInfo/MedBioInfo2025.html",
    "title": "MedBioInfo 2025",
    "section": "",
    "text": "Class of 2025!\nAfter receiving our pink unicorn T-shirts&#8482, we took our group picture and had lunch together! \n\n\nCourse Dinner\nOn Wednesday, we all (except some deserters) met at Tzatsiki for a greek dinner in the Mediterranean town of Uppsaliki!"
  }
]